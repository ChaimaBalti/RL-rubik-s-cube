{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RubiksCube2x2Env:\n",
    "    def __init__(self):\n",
    "        # Initialize the solved cube (a list of six faces)\n",
    "        self.reset()\n",
    "\n",
    "        # Define the actions: U, F, R, L, D, B and their counterclockwise counterparts\n",
    "        self.actions = ['U', \"U'\", 'F', \"F'\", 'R', \"R'\", 'L', \"L'\", 'D', \"D'\", 'B', \"B'\"]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the solved state.\n",
    "        The cube is represented as a list of 6 faces, each having 4 stickers.\n",
    "        \"\"\"\n",
    "        self.cube = [['W']*4, ['R']*4, ['G']*4, ['B']*4, ['O']*4, ['Y']*4]  # Solved state\n",
    "        self.state = self.get_state()\n",
    "        return self.state\n",
    "\n",
    "    def scramble(self, num_moves=10):\n",
    "        \"\"\"\n",
    "        Scrambles the cube by performing a random sequence of moves.\n",
    "        \"\"\"\n",
    "        for _ in range(num_moves):\n",
    "            action = random.choice(self.actions)\n",
    "            self.apply_action(action)\n",
    "        return self.get_state()\n",
    "\n",
    "    def is_solved(self):\n",
    "        \"\"\"\n",
    "        Check if the cube is solved. A solved cube has all stickers of each face the same color.\n",
    "        \"\"\"\n",
    "        return all(len(set(face)) == 1 for face in self.cube)\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Converts the cube to a tuple of tuples (to be hashable) for use as a state representation.\n",
    "        \"\"\"\n",
    "        return tuple(tuple(face) for face in self.cube)\n",
    "\n",
    "\n",
    "    # Function to apply an action to the cube (rotation)\n",
    "    def _rotate_face_clockwise(self, face):\n",
    "        #\"\"\"Rotate a 2x2 face 90 degrees clockwise.\"\"\"\n",
    "        return [face[2], face[0], face[3], face[1]]\n",
    "\n",
    "    def _rotate_face_counterclockwise(self, face):\n",
    "    #\"\"\"Rotate a 2x2 face 90 degrees counterclockwise.\"\"\"\n",
    "        return [face[1], face[3], face[0], face[2]]\n",
    "\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        if action == \"U\":\n",
    "            self._rotate_u()\n",
    "        elif action == \"U'\":\n",
    "            self._rotate_u_prime()\n",
    "        elif action == \"D\":\n",
    "            self._rotate_d()\n",
    "        elif action == \"D'\":\n",
    "            self._rotate_d_prime()\n",
    "        elif action == \"R\":\n",
    "            self._rotate_r()\n",
    "        elif action == \"R'\":\n",
    "            self._rotate_r_prime()\n",
    "        elif action == \"L\":\n",
    "            self._rotate_l()\n",
    "        elif action == \"L'\":\n",
    "            self._rotate_l_prime()\n",
    "        elif action == \"F\":\n",
    "            self._rotate_f()\n",
    "        elif action == \"F'\":\n",
    "            self._rotate_f_prime()\n",
    "        elif action == \"B\":\n",
    "            self._rotate_b()\n",
    "        elif action == \"B'\":\n",
    "            self._rotate_b_prime()\n",
    "\n",
    "    def _rotate_u(self):\n",
    "    # U: F[0,1] <-> L[0,1] <-> B[0,1] <-> R[0,1]\n",
    "        self.cube[2] = self._rotate_face_clockwise(self.cube[2])  # Top face\n",
    "        self.cube[0][0:2], self.cube[4][0:2], self.cube[1][0:2], self.cube[5][0:2] = \\\n",
    "        self.cube[4][0:2], self.cube[1][0:2], self.cube[5][0:2], self.cube[0][0:2]\n",
    "\n",
    "    def _rotate_u_prime(self):\n",
    "        self.cube[2] = self._rotate_face_counterclockwise(self.cube[2])\n",
    "        self.cube[0][0:2], self.cube[4][0:2], self.cube[1][0:2], self.cube[5][0:2] = \\\n",
    "        self.cube[5][0:2], self.cube[0][0:2], self.cube[4][0:2], self.cube[1][0:2]\n",
    "\n",
    "    def _rotate_d(self):\n",
    "        # D: F[2,3] <-> R[2,3] <-> B[2,3] <-> L[2,3]\n",
    "        self.cube[3] = self._rotate_face_clockwise(self.cube[3])  # Bottom face\n",
    "        self.cube[0][2:4], self.cube[5][2:4], self.cube[1][2:4], self.cube[4][2:4] = \\\n",
    "        self.cube[5][2:4], self.cube[1][2:4], self.cube[4][2:4], self.cube[0][2:4]\n",
    "\n",
    "    def _rotate_d_prime(self):\n",
    "        self.cube[3] = self._rotate_face_counterclockwise(self.cube[3])\n",
    "        self.cube[0][2:4], self.cube[5][2:4], self.cube[1][2:4], self.cube[4][2:4] = \\\n",
    "        self.cube[4][2:4], self.cube[0][2:4], self.cube[5][2:4], self.cube[1][2:4]\n",
    "\n",
    "    def _rotate_l(self):\n",
    "        # L: F[0,2] <-> D[0,2] <-> B[1,3] <-> U[0,2]\n",
    "        self.cube[4] = self._rotate_face_clockwise(self.cube[4])  # Left face\n",
    "        temp = [self.cube[0][0], self.cube[0][2]]\n",
    "        self.cube[0][0], self.cube[0][2] = self.cube[3][0], self.cube[3][2]\n",
    "        self.cube[3][0], self.cube[3][2] = self.cube[1][3], self.cube[1][1]\n",
    "        self.cube[1][3], self.cube[1][1] = self.cube[2][0], self.cube[2][2]\n",
    "        self.cube[2][0], self.cube[2][2] = temp\n",
    "\n",
    "    def _rotate_l_prime(self):\n",
    "        self.cube[4] = self._rotate_face_counterclockwise(self.cube[4])\n",
    "        temp = [self.cube[0][0], self.cube[0][2]]\n",
    "        self.cube[0][0], self.cube[0][2] = self.cube[2][0], self.cube[2][2]\n",
    "        self.cube[2][0], self.cube[2][2] = self.cube[1][3], self.cube[1][1]\n",
    "        self.cube[1][3], self.cube[1][1] = self.cube[3][0], self.cube[3][2]\n",
    "        self.cube[3][0], self.cube[3][2] = temp\n",
    "\n",
    "    def _rotate_r(self):\n",
    "        # R: F[1,3] <-> U[1,3] <-> B[0,2] <-> D[1,3]\n",
    "        self.cube[5] = self._rotate_face_clockwise(self.cube[5])  # Right face\n",
    "        temp = [self.cube[0][1], self.cube[0][3]]\n",
    "        self.cube[0][1], self.cube[0][3] = self.cube[2][1], self.cube[2][3]\n",
    "        self.cube[2][1], self.cube[2][3] = self.cube[1][2], self.cube[1][0]\n",
    "        self.cube[1][2], self.cube[1][0] = self.cube[3][1], self.cube[3][3]\n",
    "        self.cube[3][1], self.cube[3][3] = temp\n",
    "\n",
    "    def _rotate_r_prime(self):\n",
    "        self.cube[5] = self._rotate_face_counterclockwise(self.cube[5])\n",
    "        temp = [self.cube[0][1], self.cube[0][3]]\n",
    "        self.cube[0][1], self.cube[0][3] = self.cube[3][1], self.cube[3][3]\n",
    "        self.cube[3][1], self.cube[3][3] = self.cube[1][2], self.cube[1][0]\n",
    "        self.cube[1][2], self.cube[1][0] = self.cube[2][1], self.cube[2][3]\n",
    "        self.cube[2][1], self.cube[2][3] = temp\n",
    "\n",
    "    def _rotate_f(self):\n",
    "        # F: U[2,3] <-> L[1,3] <-> D[0,1] <-> R[0,2]\n",
    "        self.cube[0] = self._rotate_face_clockwise(self.cube[0])  # Front face\n",
    "        temp = [self.cube[2][2], self.cube[2][3]]\n",
    "        self.cube[2][2], self.cube[2][3] = self.cube[4][3], self.cube[4][1]\n",
    "        self.cube[4][3], self.cube[4][1] = self.cube[3][0], self.cube[3][1]\n",
    "        self.cube[3][0], self.cube[3][1] = self.cube[5][0], self.cube[5][2]\n",
    "        self.cube[5][0], self.cube[5][2] = temp\n",
    "\n",
    "    def _rotate_f_prime(self):\n",
    "        self.cube[0] = self._rotate_face_counterclockwise(self.cube[0])\n",
    "        temp = [self.cube[2][2], self.cube[2][3]]\n",
    "        self.cube[2][2], self.cube[2][3] = self.cube[5][0], self.cube[5][2]\n",
    "        self.cube[5][0], self.cube[5][2] = self.cube[3][0], self.cube[3][1]\n",
    "        self.cube[3][0], self.cube[3][1] = self.cube[4][3], self.cube[4][1]\n",
    "        self.cube[4][3], self.cube[4][1] = temp\n",
    "\n",
    "    def _rotate_b(self):\n",
    "        # B: U[0,1] <-> R[1,3] <-> D[2,3] <-> L[0,2]\n",
    "        self.cube[1] = self._rotate_face_clockwise(self.cube[1])  # Back face\n",
    "        temp = [self.cube[2][0], self.cube[2][1]]\n",
    "        self.cube[2][0], self.cube[2][1] = self.cube[5][1], self.cube[5][3]\n",
    "        self.cube[5][1], self.cube[5][3] = self.cube[3][2], self.cube[3][3]\n",
    "        self.cube[3][2], self.cube[3][3] = self.cube[4][0], self.cube[4][2]\n",
    "        self.cube[4][0], self.cube[4][2] = temp\n",
    "\n",
    "    def _rotate_b_prime(self):\n",
    "        self.cube[1] = self._rotate_face_counterclockwise(self.cube[1])\n",
    "        temp = [self.cube[2][0], self.cube[2][1]]\n",
    "        self.cube[2][0], self.cube[2][1] = self.cube[4][0], self.cube[4][2]\n",
    "        self.cube[4][0], self.cube[4][2] = self.cube[3][2], self.cube[3][3]\n",
    "        self.cube[3][2], self.cube[3][3] = self.cube[5][1], self.cube[5][3]\n",
    "        self.cube[5][1], self.cube[5][3] = temp\n",
    "\n",
    "    def choose_action(self, q_table, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Chooses an action based on epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Exploration: choose a random action\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            # Exploitation: choose the action with the highest Q-value\n",
    "            if self.state not in q_table:\n",
    "                q_table[self.state] = {action: 0 for action in self.actions}\n",
    "            return max(q_table[self.state], key=q_table[self.state].get)\n",
    "\n",
    "    def count_solved_faces(self):\n",
    "        \"\"\"\n",
    "        Counts the number of fully solved faces.\n",
    "        A fully solved face has all stickers of the same color.\n",
    "        \"\"\"\n",
    "        return sum(face.count(face[0]) == len(face) for face in self.cube)\n",
    "\n",
    "    def step(self, action, q_table, alpha=0.1, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Takes an action, applies it to the cube, and returns the new state, reward, and whether the episode is done.\n",
    "        \"\"\"\n",
    "        # Apply the action to get the next state\n",
    "        self.apply_action(action)\n",
    "        next_state = self.get_state()\n",
    "\n",
    "         # Calculate reward based on the number of solved faces\n",
    "        solved_faces = self.count_solved_faces()\n",
    "        if self.is_solved():\n",
    "            reward = 10  # Highest reward for solving the entire cube\n",
    "        else:\n",
    "            reward = solved_faces  # Reward for the number of solved faces (0 to 5)\n",
    "\n",
    "        # Add a small penalty for each move to encourage faster solutions\n",
    "        reward -= 0.01\n",
    "\n",
    "        # Update the Q-value using the Q-learning update rule\n",
    "        if next_state not in q_table:\n",
    "            q_table[next_state] = {action: 0 for action in self.actions}\n",
    "\n",
    "        best_next_action = max(q_table[next_state], key=q_table[next_state].get)\n",
    "        q_table[self.state][action] += alpha * (reward + gamma * q_table[next_state][best_next_action] - q_table[self.state][action])\n",
    "\n",
    "        # Move to the next state\n",
    "        self.state = next_state\n",
    "\n",
    "        # Return the next state, reward, and whether the cube is solved\n",
    "        return next_state, reward, self.is_solved()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "def train_agent(num_episodes=10000):\n",
    "    env = RubiksCube2x2Env()\n",
    "    q_table = {}\n",
    "    epsilon = 0.1  # Exploration rate\n",
    "    alpha = 0.1    # Learning rate\n",
    "    gamma = 0.9    # Discount factor\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment at the start of each episode\n",
    "        env.reset()\n",
    "        env.scramble(20)\n",
    "        step = 0\n",
    "        max_reward = - np.Infinity \n",
    "        # Reduce exploration rate over time\n",
    "        if epsilon > 0.01:\n",
    "            epsilon *= 0.995\n",
    "        while not env.is_solved() and step < 30:\n",
    "            step +=1\n",
    "            # Choose an action\n",
    "            action = env.choose_action(q_table, epsilon)\n",
    "            # Take the action and get the next state, reward, and done flag\n",
    "            next_state, reward, done = env.step(action, q_table, alpha, gamma)\n",
    "            max_reward = np.maximum(reward,max_reward)\n",
    "            if done:\n",
    "                print(\"*************************************************Solved!********************************************************\")\n",
    "                print( \"Episode n°\", episode, \"reward = \", max_reward, \"in \", step , \"steps, SOLVED :D !!!!!!\")\n",
    "                break\n",
    "        if episode % 1000000 == 0:\n",
    "          print( \"Episode n°\", episode, \"reward = \", max_reward, \"in \", step , \"steps\")\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode n° 0 reward =  -0.01 in  30 steps\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 72356 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 172043 reward =  9.99 in  20 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 179348 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 231093 reward =  9.99 in  12 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 284579 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 327875 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 334339 reward =  9.99 in  20 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 401204 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 402417 reward =  9.99 in  12 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 422623 reward =  9.99 in  10 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 451205 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 541806 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 629225 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 642093 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 991736 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "Episode n° 1000000 reward =  -0.01 in  30 steps\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1184567 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1297126 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1501141 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1529654 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1530158 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1553737 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1592956 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1608167 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1670157 reward =  9.99 in  8 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1709805 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1792448 reward =  9.99 in  22 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 1931970 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "Episode n° 2000000 reward =  -0.01 in  30 steps\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2162067 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2286554 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2343560 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2366295 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2437876 reward =  9.99 in  12 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2515292 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2759611 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2795585 reward =  9.99 in  22 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2884368 reward =  9.99 in  22 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2959113 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n",
      "*************************************************Solved!********************************************************\n",
      "Episode n° 2965363 reward =  9.99 in  2 steps, SOLVED :D !!!!!!\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training the agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m q_table \u001b[38;5;241m=\u001b[39m train_agent(\u001b[38;5;241m1000000000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(num_episodes)\u001b[0m\n\u001b[0;32m     22\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mchoose_action(q_table, epsilon)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Take the action and get the next state, reward, and done flag\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action, q_table, alpha, gamma)\n\u001b[0;32m     25\u001b[0m max_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(reward,max_reward)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[2], line 206\u001b[0m, in \u001b[0;36mRubiksCube2x2Env.step\u001b[1;34m(self, action, q_table, alpha, gamma)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Update the Q-value using the Q-learning update rule\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m q_table:\n\u001b[1;32m--> 206\u001b[0m     q_table[next_state] \u001b[38;5;241m=\u001b[39m {action: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions}\n\u001b[0;32m    208\u001b[0m best_next_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(q_table[next_state], key\u001b[38;5;241m=\u001b[39mq_table[next_state]\u001b[38;5;241m.\u001b[39mget)\n\u001b[0;32m    209\u001b[0m q_table[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate][action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m q_table[next_state][best_next_action] \u001b[38;5;241m-\u001b[39m q_table[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate][action])\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the agent\n",
    "q_table = train_agent(1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the agent on a scrambled cube\n",
    "env = RubiksCube2x2Env()\n",
    "test_cube = env.scramble(10)\n",
    "print(\"Scrambled Cube:\")\n",
    "print(test_cube)\n",
    "steps = 0\n",
    "while not env.is_solved():\n",
    "    state = env.get_state()\n",
    "    action = max(q_table[state], key=q_table[state].get)  # Choose best action\n",
    "    env.apply_action(action)\n",
    "    steps += 1\n",
    "    print(f\"Step {steps}: Action {action}\")\n",
    "print(\"Cube solved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CudafriendlyENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
